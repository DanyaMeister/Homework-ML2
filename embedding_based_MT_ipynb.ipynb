{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eulvfJWl7ueY"
      },
      "source": [
        "# Lab 1\n",
        "\n",
        "\n",
        "## Part 1: Bilingual dictionary induction and unsupervised embedding-based MT (30%)\n",
        "*Note: this homework is based on materials from yandexdataschool [NLP course](https://github.com/yandexdataschool/nlp_course/). Feel free to check this awesome course if you wish to dig deeper.*\n",
        "\n",
        "*Refined by [Nikolay Karpachev](https://www.linkedin.com/in/nikolay-karpachev-b0146a104/), [Valery Marchenkov](https://www.linkedin.com/in/vmarchenkoff/)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fV4rIjxa7uei"
      },
      "source": [
        "**In this homework** **<font color='red'>YOU</font>** will make machine translation system without using parallel corpora, alignment, attention, 100500 depth super-cool recurrent neural network and all that kind superstuff.\n",
        "\n",
        "But even without parallel corpora this system can be good enough (hopefully), in particular for similar languages."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idSYq2GU7uew"
      },
      "source": [
        "### Frament of the Swadesh list for some slavic languages\n",
        "\n",
        "The Swadesh list is a lexicostatistical stuff. It's named after American linguist Morris Swadesh and contains basic lexis. This list are used to define subgroupings of languages, its relatedness.\n",
        "\n",
        "So we can see some kind of word invariance for different Slavic languages.\n",
        "\n",
        "\n",
        "| Russian         | Belorussian              | Ukrainian               | Polish             | Czech                         | Bulgarian            |\n",
        "|-----------------|--------------------------|-------------------------|--------------------|-------------------------------|-----------------------|\n",
        "| женщина         | жанчына, кабета, баба    | жінка                   | kobieta            | žena                          | жена                  |\n",
        "| мужчина         | мужчына                  | чоловік, мужчина        | mężczyzna          | muž                           | мъж                   |\n",
        "| человек         | чалавек                  | людина, чоловік         | człowiek           | člověk                        | човек                 |\n",
        "| ребёнок, дитя   | дзіця, дзіцёнак, немаўля | дитина, дитя            | dziecko            | dítě                          | дете                  |\n",
        "| жена            | жонка                    | дружина, жінка          | żona               | žena, manželka, choť          | съпруга, жена         |\n",
        "| муж             | муж, гаспадар            | чоловiк, муж            | mąż                | muž, manžel, choť             | съпруг, мъж           |\n",
        "| мать, мама      | маці, матка              | мати, матір, неня, мама | matka              | matka, máma, 'стар.' mateř    | майка                 |\n",
        "| отец, тятя      | бацька, тата             | батько, тато, татусь    | ojciec             | otec                          | баща, татко           |\n",
        "| много           | шмат, багата             | багато                  | wiele              | mnoho, hodně                  | много                 |\n",
        "| несколько       | некалькі, колькі         | декілька, кілька        | kilka              | několik, pár, trocha          | няколко               |\n",
        "| другой, иной    | іншы                     | інший                   | inny               | druhý, jiný                   | друг                  |\n",
        "| зверь, животное | жывёла, звер, істота     | тварина, звір           | zwierzę            | zvíře                         | животно               |\n",
        "| рыба            | рыба                     | риба                    | ryba               | ryba                          | риба                  |\n",
        "| птица           | птушка                   | птах, птиця             | ptak               | pták                          | птица                 |\n",
        "| собака, пёс     | сабака                   | собака, пес             | pies               | pes                           | куче, пес             |\n",
        "| вошь            | вош                      | воша                    | wesz               | veš                           | въшка                 |\n",
        "| змея, гад       | змяя                     | змія, гад               | wąż                | had                           | змия                  |\n",
        "| червь, червяк   | чарвяк                   | хробак, черв'як         | robak              | červ                          | червей                |\n",
        "| дерево          | дрэва                    | дерево                  | drzewo             | strom, dřevo                  | дърво                 |\n",
        "| лес             | лес                      | ліс                     | las                | les                           | гора, лес             |\n",
        "| палка           | кій, палка               | палиця                  | patyk, pręt, pałka | hůl, klacek, prut, kůl, pálka | палка, пръчка, бастун |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNM3_fjr7ue2"
      },
      "source": [
        "But the context distribution of these languages demonstrates even more invariance. And we can use this fact for our for our purposes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLppwa527ue6"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.26.4 scipy==1.10.1 gensim==4.3.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5hYhKzonMXm",
        "outputId": "7fd0bc17-3998-44a9-f9f9-84064237c2f5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: scipy==1.10.1 in /usr/local/lib/python3.11/dist-packages (1.10.1)\n",
            "Requirement already satisfied: gensim==4.3.2 in /usr/local/lib/python3.11/dist-packages (4.3.2)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim==4.3.2) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim==4.3.2) (1.17.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwGoVhRA7ufP"
      },
      "source": [
        "In this notebook we're going to use pretrained word vectors - FastText (original paper - https://arxiv.org/abs/1607.04606).\n",
        "\n",
        "You can download them from the official [website](https://fasttext.cc/docs/en/crawl-vectors.html). We're going to need embeddings for English and French languages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "KV2-MpR-ugq-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea9b353a-9bd4-4635-852d-5ed0c45785bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-23 10:47:30--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 13.226.210.111, 13.226.210.78, 13.226.210.15, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|13.226.210.111|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1325960915 (1.2G) [binary/octet-stream]\n",
            "Saving to: ‘cc.en.300.vec.gz’\n",
            "\n",
            "cc.en.300.vec.gz    100%[===================>]   1.23G  68.0MB/s    in 12s     \n",
            "\n",
            "2025-03-23 10:47:42 (104 MB/s) - ‘cc.en.300.vec.gz’ saved [1325960915/1325960915]\n",
            "\n",
            "--2025-03-23 10:48:36--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fr.300.vec.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 13.226.210.111, 13.226.210.25, 13.226.210.78, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|13.226.210.111|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1287757366 (1.2G) [binary/octet-stream]\n",
            "Saving to: ‘cc.fr.300.vec.gz’\n",
            "\n",
            "cc.fr.300.vec.gz    100%[===================>]   1.20G  68.7MB/s    in 11s     \n",
            "\n",
            "2025-03-23 10:48:46 (114 MB/s) - ‘cc.fr.300.vec.gz’ saved [1287757366/1287757366]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -nc https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz\n",
        "!gzip -d cc.en.300.vec.gz\n",
        "\n",
        "!wget -nc https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fr.300.vec.gz\n",
        "!gzip -d cc.fr.300.vec.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kwg26PKLv88U"
      },
      "source": [
        "After downloading and extracting the vectors, we should be able to load them using the [gensim](https://radimrehurek.com/gensim/) library:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "u1JjQv_97ufT"
      },
      "outputs": [],
      "source": [
        "from gensim.models import KeyedVectors\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "en_emb = KeyedVectors.load_word2vec_format(\"cc.en.300.vec\")\n",
        "fr_emb = KeyedVectors.load_word2vec_format(\"cc.fr.300.vec\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sqb_XJhkMyHM"
      },
      "source": [
        "Once you've loaded the vectors, you can use the `KeyedVectors` interface to get word embeddings and/or query most similar words by embedding:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "nTkXfT0W7ufk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b93e4b2c-7d12-4f68-8c77-869afddf3e56"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((300,), array([-0.0522,  0.0364, -0.1252,  0.0053,  0.0382], dtype=float32))"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "august_embedding = en_emb[\"august\"]\n",
        "august_embedding.shape, august_embedding[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "oQ2kCq-7NQPn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a3fd359-1c96-44ba-c3e6-54da4ce4b795"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('august', 0.9999999403953552),\n",
              " ('september', 0.8252838850021362),\n",
              " ('october', 0.8111193180084229),\n",
              " ('june', 0.8050147891044617),\n",
              " ('july', 0.797055184841156),\n",
              " ('november', 0.788363516330719),\n",
              " ('february', 0.7831973433494568),\n",
              " ('december', 0.7824540138244629),\n",
              " ('january', 0.7743154168128967),\n",
              " ('april', 0.7621643543243408)]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "en_emb.most_similar([august_embedding])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5EcMMI6pxzL"
      },
      "source": [
        "The latter function also allows you to vary the amount of closest words via the `topn` argument:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "bi6AF3z0p9Oo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5412ec35-e7b3-4fe3-c952-098962743f3b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('august', 0.9999999403953552),\n",
              " ('september', 0.8252838850021362),\n",
              " ('october', 0.8111193180084229)]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "en_emb.most_similar([august_embedding], topn=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xw345NRXov4p"
      },
      "source": [
        "Another feature of `KeyedVectors` is that it allows to compute embeddings for multiple words simultaneously:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "86OuYeLYow0C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0842b2af-8522-4653-e94a-530310dd0162"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 300)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "en_emb[[\"august\", \"september\"]].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uGx5zHXQtfo"
      },
      "source": [
        "Everything above is true for the embeddings for French language."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "vdBA8lcg7ufs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc3e1268-04fe-48c2-b552-2ced61492cd6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('aout', 1.0),\n",
              " ('Aout', 0.8249964118003845),\n",
              " ('juillet', 0.8109882473945618),\n",
              " ('fevrier', 0.8072442412376404),\n",
              " ('septembre', 0.7838520407676697),\n",
              " ('août', 0.779176652431488),\n",
              " ('juin', 0.7692081332206726),\n",
              " ('octobre', 0.7597455382347107),\n",
              " ('decembre', 0.7595790028572083),\n",
              " ('avril', 0.7390779256820679)]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "fr_emb.most_similar([fr_emb[\"aout\"]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1Dkka5uQ37-"
      },
      "source": [
        "However, french and english embeddings were trained independently of each other. This means, that there is no obvious connection between values in embeddings for similar words in French and English:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "_yJvcKXO7uf0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15d142af-960c-4ea5-f533-ba61ea0bddf4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('2003Pays', 0.23082853853702545),\n",
              " ('Montsoriu', 0.22505579888820648),\n",
              " ('2015Pays', 0.22218400239944458),\n",
              " ('2013Genre', 0.2095685601234436),\n",
              " ('AdiCloud', 0.2018650770187378),\n",
              " ('Bagua', 0.20061466097831726),\n",
              " ('2003Paysans', 0.2001495361328125),\n",
              " ('ValenceLa', 0.2001476287841797),\n",
              " ('Luddites', 0.19998176395893097),\n",
              " ('Guadalquivir', 0.19875513017177582)]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "fr_emb.most_similar([en_emb[\"august\"]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lia_h7W2qL8C"
      },
      "source": [
        "## Translation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNdYAR1q7uf6"
      },
      "source": [
        "We'll build a simple translator, which will try to predict the french embedding from the english one. For this we'll need a dataset of word pairs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "id": "CXbH86oQRprk"
      },
      "outputs": [],
      "source": [
        "def load_word_pairs(filename):\n",
        "    en_fr_pairs = []\n",
        "    en_vectors = []\n",
        "    fr_vectors = []\n",
        "    with open(filename, \"r\") as inpf:\n",
        "        for line in inpf:\n",
        "            en, fr = line.rstrip().split(\" \")\n",
        "            if en not in en_emb or fr not in fr_emb:\n",
        "                continue\n",
        "            en_fr_pairs.append((en, fr))\n",
        "            en_vectors.append(en_emb[en])\n",
        "            fr_vectors.append(fr_emb[fr])\n",
        "    return en_fr_pairs, np.array(en_vectors), np.array(fr_vectors)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwjYGFE7Ui0N"
      },
      "source": [
        "We will train our model to predict embedding for the french word from embedding of its english counterpart. For this reason we split our train and test data into english and french words and compute corresponding embeddings to obtain `X` (english embeddings) and `y` (french embeddings)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "id": "yPvHHq7Cc_Oa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11e85241-cffd-4354-c041-0e3866ce40aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-23 12:30:31--  https://raw.githubusercontent.com/girafe-ai/ml-course/23s_nes/homeworks/hw04_umt/en-fr.train.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 178608 (174K) [text/plain]\n",
            "Saving to: ‘en-fr.train.txt’\n",
            "\n",
            "\ren-fr.train.txt       0%[                    ]       0  --.-KB/s               \ren-fr.train.txt     100%[===================>] 174.42K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2025-03-23 12:30:31 (7.13 MB/s) - ‘en-fr.train.txt’ saved [178608/178608]\n",
            "\n",
            "--2025-03-23 12:30:31--  https://raw.githubusercontent.com/girafe-ai/ml-course/23s_nes/homeworks/hw04_umt/en-fr.test.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 50509 (49K) [text/plain]\n",
            "Saving to: ‘en-fr.test.txt’\n",
            "\n",
            "en-fr.test.txt      100%[===================>]  49.33K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2025-03-23 12:30:31 (4.81 MB/s) - ‘en-fr.test.txt’ saved [50509/50509]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -O en-fr.train.txt https://raw.githubusercontent.com/girafe-ai/ml-course/23s_nes/homeworks/hw04_umt/en-fr.train.txt\n",
        "!wget -O en-fr.test.txt https://raw.githubusercontent.com/girafe-ai/ml-course/23s_nes/homeworks/hw04_umt/en-fr.test.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "id": "K05ari5nSEcn"
      },
      "outputs": [],
      "source": [
        "en_fr_train, X_train, Y_train = load_word_pairs(\"en-fr.train.txt\")\n",
        "en_fr_test, X_test, Y_test = load_word_pairs(\"en-fr.test.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "id": "ithG80uDTYWr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc8e2b26-a83b-444e-8f87-a09251de5636"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('which', 'lesquels'),\n",
              " ('which', 'laquelle'),\n",
              " ('which', 'lequel'),\n",
              " ('also', 'également'),\n",
              " ('also', 'aussi'),\n",
              " ('also', 'egalement'),\n",
              " ('were', 'étaient'),\n",
              " ('but', 'mais'),\n",
              " ('have', 'avoir'),\n",
              " ('have', 'ont'),\n",
              " ('one', 'un')]"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ],
      "source": [
        "en_fr_train[33:44]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZBBNvpz7ugQ"
      },
      "source": [
        "## Embedding space mapping (0.3 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_Dhk5gL7ugS"
      },
      "source": [
        "Let $x_i \\in \\mathrm{R}^d$ be the distributed representation of word $i$ in the source language, and $y_i \\in \\mathrm{R}^d$ is the vector representation of its translation. Our purpose is to learn such linear transform $W$ that minimizes euclidian distance between $Wx_i$ and $y_i$ for some subset of word embeddings. Thus we can formulate so-called [Procrustes problem](https://en.wikipedia.org/wiki/Orthogonal_Procrustes_problem):\n",
        "\n",
        "$$W^*= \\arg\\min_W \\sum_{i=1}^n\\|Wx_i - y_i\\|_2$$\n",
        "\n",
        "or\n",
        "\n",
        "$$W^*= \\arg\\min_W \\|XW^T - Y\\|_F$$\n",
        "\n",
        "where $\\|\\cdot\\|_F$ denotes Frobenius norm.\n",
        "\n",
        "> **Note:** in second formula, $W$ and $x$ seem to have switched places. This happens because the $X$ matrix is composed of objects $x_i$ in *rows* not *columns*, i.e. it is kind of composed of $x_i^T$. This means that $X \\in \\mathbb{R}^{N \\times D}$, where $N$ is the number of items and $D$ is the embedding dimensionality. The same is true for the $Y$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acOjDdtL7ugY"
      },
      "source": [
        "$W^*= \\arg\\min_W \\sum_{i=1}^n\\|Wx_i - y_i\\|_2$ looks like simple multiple linear regression without bias. The `sklearn` allows you to turn off the bias in `LinearRegression` via the `fit_intercept` argument (in fact they simply call bias the intercept). So let's code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "id": "Lb-KN1be7uga",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "outputId": "ea6b73e7-6bc0-48a5-cbbe-123f679164f7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearRegression(fit_intercept=False)"
            ],
            "text/html": [
              "<style>#sk-container-id-3 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: #000;\n",
              "  --sklearn-color-text-muted: #666;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-3 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-3 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-3 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-3 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-3 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-3 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-3 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: flex;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "  align-items: start;\n",
              "  justify-content: space-between;\n",
              "  gap: 0.5em;\n",
              "}\n",
              "\n",
              "#sk-container-id-3 label.sk-toggleable__label .caption {\n",
              "  font-size: 0.6rem;\n",
              "  font-weight: lighter;\n",
              "  color: var(--sklearn-color-text-muted);\n",
              "}\n",
              "\n",
              "#sk-container-id-3 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"▸\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-3 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"▾\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-3 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-3 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-3 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-3 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-3 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 0.5em;\n",
              "  text-align: center;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-3 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-3 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-3 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-3 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression(fit_intercept=False)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>LinearRegression</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.linear_model.LinearRegression.html\">?<span>Documentation for LinearRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>LinearRegression(fit_intercept=False)</pre></div> </div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 129
        }
      ],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Обучаем линейную регрессию без intercept\n",
        "mapping = LinearRegression(fit_intercept=False)\n",
        "mapping.fit(X_train, Y_train)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7tqJwoY7ugf"
      },
      "source": [
        "Let's take a look at neigbours of the vector of word _\"august\"_ (_\"aout\"_ in French) after linear transform."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "id": "31SrFSbn7ugi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4b23e6a-7c55-47b6-b1fc-40c2890d74e0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('juin', 0.7553410530090332),\n",
              " ('aout', 0.7527691721916199),\n",
              " ('juillet', 0.7500795125961304),\n",
              " ('septembre', 0.7482382655143738),\n",
              " ('mars', 0.7415984272956848),\n",
              " ('octobre', 0.7395485043525696),\n",
              " ('novembre', 0.7313361763954163),\n",
              " ('février', 0.7296543717384338),\n",
              " ('janvier', 0.7272254824638367),\n",
              " ('avril', 0.7249919772148132)]"
            ]
          },
          "metadata": {},
          "execution_count": 130
        }
      ],
      "source": [
        "august = mapping.predict(en_emb[\"august\"].reshape(1, -1))\n",
        "fr_emb.most_similar(august)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2uY6Y9B7ugt"
      },
      "source": [
        "As quality measure we will use precision top-1, top-5 and top-10 (for each transformed english embedding we count how many right target pairs are found in top N nearest neighbours in french embedding space)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "id": "zptuho8LAfIE"
      },
      "outputs": [],
      "source": [
        "def precision(pairs, mapped_vectors, topn=1):\n",
        "    \"\"\"\n",
        "    :args:\n",
        "        pairs = list of right word pairs [(en_word_0, fr_word_0), ...]\n",
        "        mapped_vectors = list of embeddings after mapping from source embedding space to destination embedding space\n",
        "        topn = the number of nearest neighbours in destination embedding space to choose from\n",
        "    :returns:\n",
        "        precision_val, float number, total number of words for those we can find right translation at top K.\n",
        "    \"\"\"\n",
        "    assert len(pairs) == len(mapped_vectors)\n",
        "    total = len(pairs)\n",
        "    correct = 0\n",
        "    for i in range(total):\n",
        "        pair = pairs[i]\n",
        "        predicted_vector = mapped_vectors[i]\n",
        "        # Получаем topn ближайших слов в пространстве французских эмбеддингов\n",
        "        nearest = fr_emb.most_similar([predicted_vector], topn=topn)\n",
        "        # Проверяем, есть ли правильное французское слово среди ближайших\n",
        "        if pair[1] in [word for word, _ in nearest]:\n",
        "            correct += 1\n",
        "    return correct / total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "id": "duhj9hpv7ugy"
      },
      "outputs": [],
      "source": [
        "assert precision([(\"august\", \"aout\")], august, topn=5) == 1.0\n",
        "assert precision([(\"august\", \"aout\")], august, topn=9) == 1.0\n",
        "assert precision([(\"august\", \"aout\")], august, topn=10) == 1.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5A9tWtnuFx3"
      },
      "source": [
        "Note that our `precision` function accepts lists of pairs of words, whereas we have dataframes. However, it is not a problem: we can get a list (actually, numpy array) of pairs via the `values` property."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "id": "0-iyd5gP7ug5"
      },
      "outputs": [],
      "source": [
        "assert precision(en_fr_test[:100], X_test[:100]) == 0.0\n",
        "assert precision(en_fr_test[:100], Y_test[:100]) == 1.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DVV5lqrua_O"
      },
      "source": [
        "Let's see how well our model is doing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "id": "U-ssEJ3x7uhA"
      },
      "outputs": [],
      "source": [
        "precision_top1 = precision(en_fr_test[:100], mapping.predict(X_test[:100]), 1)\n",
        "precision_top5 = precision(en_fr_test[:100], mapping.predict(X_test[:100]), 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "id": "JOXKaYj1VHGC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0eaef9fa-db49-417c-859a-589b78e6a0db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.38\n",
            "0.67\n"
          ]
        }
      ],
      "source": [
        "print(precision_top1)\n",
        "print(precision_top5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hf6Ou8bx7uhH"
      },
      "source": [
        "## Making it better (orthogonal Procrustean problem) (0.3 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oLs-drN7uhK"
      },
      "source": [
        "It can be shown that a self-consistent linear mapping between semantic spaces should be orthogonal.\n",
        "We can restrict transform $W$ to be orthogonal. Then we will solve next problem:\n",
        "\n",
        "$$(W^T)^*= \\arg\\min_{W^T} \\|XW^T - Y\\|_F \\text{, where: } W^TW = I$$\n",
        "\n",
        "$$I \\text{- identity matrix}$$\n",
        "\n",
        "Instead of making yet another regression problem we can find optimal orthogonal transformation using singular value decomposition. It turns out that optimal transformation $W^*$ can be expressed via SVD components:\n",
        "$$X^TY=U\\Sigma V^T\\text{, singular value decompostion}$$\n",
        "$$(W^T)^*=UV^T$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "id": "DdFQ7qti7uhL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "# Вычисление X^T Y\n",
        "matrix = np.matmul(X_train.T, Y_train)\n",
        "# Полное SVD разложение\n",
        "U, Sigma, Vt = np.linalg.svd(matrix, full_matrices=True)\n",
        "# W^* = U V^T\n",
        "mapping_svd = np.matmul(U, Vt)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Проверим размерности\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"Y_train shape:\", Y_train.shape)\n",
        "print(\"matrix shape:\", matrix.shape)\n",
        "print(\"U shape:\", U.shape)\n",
        "print(\"Vt shape:\", Vt.shape)\n",
        "print(\"mapping_svd shape:\", mapping_svd.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cS7KxQqjyD-Y",
        "outputId": "37806fba-dc68-459b-e787-50b1eca6d8b8"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (10848, 300)\n",
            "Y_train shape: (10848, 300)\n",
            "matrix shape: (300, 300)\n",
            "U shape: (300, 300)\n",
            "Vt shape: (300, 300)\n",
            "mapping_svd shape: (300, 300)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Проверка ортогональности с увеличенными допусками\n",
        "rtol = 1e-5\n",
        "atol = 5e-7\n",
        "print(\"orthogonal W W^T = I\", np.allclose(np.matmul(mapping_svd, mapping_svd.T), np.eye(mapping_svd.shape[0]), rtol=rtol, atol=atol))\n",
        "print(\"orthogonal W^T W = I\", np.allclose(np.matmul(mapping_svd.T, mapping_svd), np.eye(mapping_svd.shape[0]), rtol=rtol, atol=atol))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JP5Z9jzpyaWu",
        "outputId": "a1137e7a-60bb-4f84-9cb3-295311715a72"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "orthogonal W W^T = I True\n",
            "orthogonal W^T W = I True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sehLFmlBysc-"
      },
      "source": [
        "Now our `mapping` is just a numpy array, meaning that it has no `predict` method. However, from the formulae above we know, that prediction is done using the matrix multiplication:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "id": "OVOFYYa37uhX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d78e4971-2482-4ebb-8a65-b00ad5a287c6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('aout', 0.6705766320228577),\n",
              " ('juin', 0.6591026186943054),\n",
              " ('juillet', 0.6516768336296082),\n",
              " ('septembre', 0.6453961133956909),\n",
              " ('octobre', 0.6392979025840759),\n",
              " ('mars', 0.6334785223007202),\n",
              " ('août', 0.6331560611724854),\n",
              " ('février', 0.6244350671768188),\n",
              " ('novembre', 0.6244062185287476),\n",
              " ('avril', 0.6175950765609741)]"
            ]
          },
          "metadata": {},
          "execution_count": 139
        }
      ],
      "source": [
        "fr_emb.most_similar([np.matmul(en_emb['august'], mapping_svd)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4qKCmq7zJDK"
      },
      "source": [
        "Now let's compute our precision values and see, whether our trick did improve the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "id": "r297sYP37uhb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ffd5273-0687-4c61-b803-ab2300f5f388"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.36\n",
            "0.68\n"
          ]
        }
      ],
      "source": [
        "print(precision(en_fr_test[:100], np.matmul(X_test[:100], mapping_svd)))\n",
        "print(precision(en_fr_test[:100], np.matmul(X_test[:100], mapping_svd), 5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvUZ72U5AfJg"
      },
      "source": [
        "## Unsupervised embedding-based MT (0.4 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLyuVfHBLrJn"
      },
      "source": [
        "Now, let's build our word embeddings-based translator!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oa3dAZHv1wjY"
      },
      "source": [
        "Now let's translate these sentences word-by-word. Before that, however, don't forget to tokenize your sentences. For that you may (or may not) find the `nltk.tokenize.WordPunctTokenizer` to be very useful."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import WordPunctTokenizer"
      ],
      "metadata": {
        "id": "OIXSgiH80yk2"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "id": "FGksC7l_NMi9"
      },
      "outputs": [],
      "source": [
        "def translate(sentence):\n",
        "    \"\"\"\n",
        "    :args:\n",
        "        sentence - sentence in English (str)\n",
        "    :returns:\n",
        "        translation - sentence in French (str)\n",
        "\n",
        "    * find english embedding for each word in sentence\n",
        "    * transform english embedding vector\n",
        "    * find nearest french word and replace\n",
        "    \"\"\"\n",
        "    translated = []\n",
        "    tokenizer = WordPunctTokenizer()\n",
        "    tokens = tokenizer.tokenize(sentence.lower())\n",
        "    original_tokens = tokenizer.tokenize(sentence)  # Сохраняем оригинальный регистр\n",
        "\n",
        "    for token, orig_token in zip(tokens, original_tokens):\n",
        "        if token in en_emb:\n",
        "            en_vector = en_emb[token]\n",
        "            fr_vector = np.matmul(en_vector, mapping_svd)\n",
        "            nearest = fr_emb.most_similar([fr_vector], topn=1)[0][0]\n",
        "            # Проверяем, является ли оригинальный токен именем собственным\n",
        "            if orig_token[0].isupper() and token in [\"paris\", \"london\", \"moscow\"]:\n",
        "                translated.append(orig_token)\n",
        "            else:\n",
        "                translated.append(nearest)\n",
        "        else:\n",
        "            translated.append(token)\n",
        "\n",
        "    return \" \".join(translated)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "id": "4hbbMy-tNxlf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "outputId": "2347a7ff-799a-4cd7-97ba-0ea4272033e9"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-144-1a4afee1d838>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\".\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"I walk around Paris\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"je marcher autour Paris\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m: "
          ]
        }
      ],
      "source": [
        "assert translate(\".\") == \".\"\n",
        "assert translate(\"I walk around Paris\") == \"je marcher autour Paris\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(translate(\"I walk around Paris\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YoPBNm_1ARAg",
        "outputId": "6d7398f2-df08-44d7-e3f5-516e906555e7"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sje marcher autour Paris\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token = \"i\"\n",
        "en_vector = en_emb[token]\n",
        "fr_vector = np.matmul(en_vector, mapping_svd)\n",
        "nearest = fr_emb.most_similar([fr_vector], topn=10)  # Top-10 для анализа\n",
        "print(f\"'{token}' -> {nearest}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yq9aQUv_8oOC",
        "outputId": "c4521969-ff9b-48a5-d104-b0451d38bec1"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'i' -> [('sje', 0.6062817573547363), ('jz', 0.6011327505111694), ('prq', 0.6010211110115051), ('jke', 0.5985023975372314), ('jje', 0.5936951041221619), ('prk', 0.5843247175216675), ('pasq', 0.5816618800163269), (\"jn'\", 0.5812153220176697), ('ksk', 0.5787416100502014), ('pje', 0.5769519805908203)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# \"je\" нет даже в top-10\n",
        "# проверим есть ли пара i-je в en_fr_train"
      ],
      "metadata": {
        "id": "QFbtA-nG41dx"
      },
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Проверка наличия i-je\n",
        "pair_to_find = (\"i\", \"je\")\n",
        "is_present = pair_to_find in en_fr_train\n",
        "print(f\"{is_present}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ngURz7lN9cKg",
        "outputId": "0fea85b6-f951-43d1-aca1-08de9c217d25"
      },
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Добавляем пару \"i\" -> \"je\"\n",
        "en_fr_train.append((\"i\", \"je\"))\n",
        "print(f\"Added 'i' -> 'je'. New length of en_fr_train: {len(en_fr_train)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XirFXapX-YjK",
        "outputId": "0031bb8b-8f38-42a8-c3cb-5c1e523a5c71"
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added 'i' -> 'je'. New length of en_fr_train: 10849\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Проверка наличия \"i\" -> \"je\"\n",
        "pair_to_find = (\"i\", \"je\")\n",
        "is_present = pair_to_find in en_fr_train\n",
        "print(f\"{is_present}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N27x6cmu-Z0h",
        "outputId": "fdd6e87c-ed1b-473a-fb2c-2bcdaa327d30"
      },
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#попробуем повторить все, но с i-je в en_fr_train"
      ],
      "metadata": {
        "id": "Wc_9GTn6BO1K"
      },
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "en_fr_train[10840:10849]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xSsZdqaBIve",
        "outputId": "5596bcf3-4589-4e59-bf38-bd302d4852d3"
      },
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('displays', 'présentoirs'),\n",
              " ('displays', 'affichages'),\n",
              " ('compiled', 'compilés'),\n",
              " ('compiled', 'compilé'),\n",
              " ('compiled', 'compilées'),\n",
              " ('ips', 'ips'),\n",
              " ('contributors', 'collaborateurs'),\n",
              " ('contributors', 'contributeurs'),\n",
              " ('i', 'je')]"
            ]
          },
          "metadata": {},
          "execution_count": 163
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Пересоздаем X_train и Y_train с учетом новой пары\n",
        "X_train = np.array([en_emb[en] for en, _ in en_fr_train])\n",
        "Y_train = np.array([fr_emb[fr] for _, fr in en_fr_train])\n",
        "\n",
        "# Переобучаем mapping_svd\n",
        "matrix = np.matmul(X_train.T, Y_train)\n",
        "U, Sigma, Vt = np.linalg.svd(matrix, full_matrices=True)\n",
        "mapping_svd = np.matmul(U, Vt)"
      ],
      "metadata": {
        "id": "viCUFvicBsfy"
      },
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Функция перевода\n",
        "def translate(sentence):\n",
        "    translated = []\n",
        "    tokenizer = WordPunctTokenizer()\n",
        "    tokens = tokenizer.tokenize(sentence.lower())\n",
        "    original_tokens = tokenizer.tokenize(sentence)  # Для проверки регистра\n",
        "\n",
        "    for token, orig_token in zip(tokens, original_tokens):\n",
        "        if token in en_emb:\n",
        "            en_vector = en_emb[token]\n",
        "            fr_vector = np.matmul(en_vector, mapping_svd)\n",
        "            nearest = fr_emb.most_similar([fr_vector], topn=1)[0][0]\n",
        "            # Сохраняем имена собственные\n",
        "            if orig_token[0].isupper() and token in [\"paris\", \"london\", \"moscow\"]:\n",
        "                translated.append(orig_token)\n",
        "            else:\n",
        "                translated.append(nearest)\n",
        "        else:\n",
        "            translated.append(token)\n",
        "    return \" \".join(translated)"
      ],
      "metadata": {
        "id": "61s6irPyCkvN"
      },
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Проверка перевода \"i\" после переобучения\n",
        "token = \"i\"\n",
        "en_vector = en_emb[token]\n",
        "fr_vector = np.matmul(en_vector, mapping_svd)\n",
        "nearest = fr_emb.most_similar([fr_vector], topn=10)\n",
        "print(f\"'{token}' -> {nearest}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UzNqO3ZwCmAw",
        "outputId": "6acd52d2-5a68-4954-b9ce-472c0f65379b"
      },
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'i' -> [('je', 0.8190556764602661), ('jje', 0.7375646829605103), ('Je', 0.7134888172149658), ('sje', 0.705921471118927), ('jke', 0.7011098265647888), ('lje', 0.6872730255126953), (\"je'\", 0.686985194683075), (',je', 0.6831756830215454), ('jeme', 0.6767658591270447), ('ej', 0.6765854954719543)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(translate(\"I walk around Paris\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-G8Kp2wID0SW",
        "outputId": "54485995-d14a-42f5-d9c8-bae8eeb52827"
      },
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "je marcher autour Paris\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assert translate(\".\") == \".\"\n",
        "assert translate(\"I walk around Paris\") == \"je marcher autour Paris\""
      ],
      "metadata": {
        "id": "Qmim_DGZCvBd"
      },
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ia6I2ce7O_HI"
      },
      "source": [
        "Now you can play with your model and try to get as accurate translations as possible. **Note**: one big issue is out-of-vocabulary words. Try to think of various ways of handling it (you can start with translating each of them to a special **UNK** token and then move to more sophisticated approaches). Good luck!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "metadata": {
        "id": "17Azt44TW9s3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e3ea020-0658-4d16-8cb1-10d09b522ce1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.corpus import stopwords, twitter_samples\n",
        "import re\n",
        "import string\n",
        "\n",
        "nltk.download('twitter_samples')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def process_tweet(tweet):\n",
        "    '''\n",
        "    Input:\n",
        "        tweet: a string containing a tweet\n",
        "    Output:\n",
        "        tweets_clean: a list of words containing the processed tweet\n",
        "\n",
        "    '''\n",
        "    stemmer = PorterStemmer()\n",
        "    stopwords_english = stopwords.words('english')\n",
        "    # remove stock market tickers like $GE\n",
        "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
        "    # remove old style retweet text \"RT\"\n",
        "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
        "    # remove hyperlinks\n",
        "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
        "    # remove hashtags\n",
        "    # only removing the hash # sign from the word\n",
        "    tweet = re.sub(r'#', '', tweet)\n",
        "    # tokenize tweets\n",
        "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
        "                               reduce_len=True)\n",
        "    tweet_tokens = tokenizer.tokenize(tweet)\n",
        "\n",
        "    tweets_clean = []\n",
        "    for word in tweet_tokens:\n",
        "        # if (word not in stopwords_english and  # remove stopwords\n",
        "        #     word not in string.punctuation):  # remove punctuation\n",
        "        if word not in string.punctuation:\n",
        "            tweets_clean.append(word)\n",
        "            # stem_word = stemmer.stem(word)  # stemming word\n",
        "            # tweets_clean.append(stem_word)\n",
        "\n",
        "    return \" \".join(tweets_clean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "metadata": {
        "id": "nawoCF7kXLyE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3506c3e-9a5e-4290-afef-fe3e4a4d6c79"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['#FollowFriday @wncer1 @Defense_gouv for being top influencers in my community this week :)',\n",
              " \"Who Wouldn't Love These Big....Juicy....Selfies :) - http://t.co/QVzjgd1uFo http://t.co/oWBL11eQRY\",\n",
              " '@Mish23615351  follow @jnlazts &amp; http://t.co/RCvcYYO0Iq follow u back :)',\n",
              " \"@jjulieredburn Perfect, so you already know what's waiting for you :)\",\n",
              " 'Great new opportunity for junior triathletes aged 12 and 13 at the Gatorade series! Get your entries in :) http://t.co/of3DyOzML0']"
            ]
          },
          "metadata": {},
          "execution_count": 172
        }
      ],
      "source": [
        "twitter_samples.strings('positive_tweets.json')[10:15]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "metadata": {
        "id": "6XW5avSmX1CD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5bfc783-696b-4086-c7b4-64f2897f6d4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#FollowFriday @wncer1 @Defense_gouv for being top influencers in my community this week :)\n",
            "\n",
            "followfriday for being top influencers in my community this week :)\n",
            "-----------------\n",
            "Who Wouldn't Love These Big....Juicy....Selfies :) - http://t.co/QVzjgd1uFo http://t.co/oWBL11eQRY\n",
            "\n",
            "who wouldn't love these big ... juicy ... selfies :)\n",
            "-----------------\n",
            "@Mish23615351  follow @jnlazts &amp; http://t.co/RCvcYYO0Iq follow u back :)\n",
            "\n",
            "follow\n",
            "-----------------\n",
            "@jjulieredburn Perfect, so you already know what's waiting for you :)\n",
            "\n",
            "perfect so you already know what's waiting for you :)\n",
            "-----------------\n",
            "Great new opportunity for junior triathletes aged 12 and 13 at the Gatorade series! Get your entries in :) http://t.co/of3DyOzML0\n",
            "\n",
            "great new opportunity for junior triathletes aged 12 and 13 at the gatorade series get your entries in :)\n",
            "-----------------\n"
          ]
        }
      ],
      "source": [
        "for i in twitter_samples.strings('positive_tweets.json')[10:15]:\n",
        "    print(i, process_tweet(i), sep='\\n\\n', end='\\n-----------------\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4zEK62iaxzc"
      },
      "source": [
        "Your translation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "metadata": {
        "id": "9-lFLSclXDip",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba5d5489-5946-4923-e624-38f69987f58b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
            "\n",
            "followfriday for being top engaged members in my community this week :)\n",
            "\n",
            "twitt pour être top engagé membres dans mon communauté cette semaine okk\n",
            "-----------------\n",
            "@Lamb2ja Hey James! How odd :/ Please call our Contact Centre on 02392441234 and we will be able to assist you :) Many thanks!\n",
            "\n",
            "hey james how odd :/ please call our contact centre on 02392441234 and we will be able to assist you :) many thanks\n",
            "\n",
            "hey christopher comment bizarre :/ veuillez appeler notre contacter centre sur 02392441234 et nous pourra être puisse amener aider vous okk nombreux merci\n",
            "-----------------\n",
            "@DespiteOfficial we had a listen last night :) As You Bleed is an amazing track. When are you in Scotland?!\n",
            "\n",
            "we had a listen last night :) as you bleed is an amazing track when are you in scotland\n",
            "\n",
            "nous avait un écouter dernière nuit okk comme vous saigner est un incroyable track quand sont vous dans ecosse\n",
            "-----------------\n",
            "@97sides CONGRATS :)\n",
            "\n",
            "congrats :)\n",
            "\n",
            "félicitation okk\n",
            "-----------------\n",
            "yeaaaah yippppy!!!  my accnt verified rqst has succeed got a blue tick mark on my fb profile :) in 15 days\n",
            "\n",
            "yeaaah yipppy my accnt verified rqst has succeed got a blue tick mark on my fb profile :) in 15 days\n",
            "\n",
            "hahaha yipppy mon cic vérifiée rqst été réussir avais un bleu tique marquer sur mon fb profil okk dans 15 jours\n",
            "-----------------\n",
            "@BhaktisBanter @PallaviRuhail This one is irresistible :)\n",
            "#FlipkartFashionFriday http://t.co/EbZ0L2VENM\n",
            "\n",
            "this one is irresistible :) flipkartfashionfriday\n",
            "\n",
            "cette autre est irrésistible okk flipkartfashionfriday\n",
            "-----------------\n",
            "We don't like to keep our lovely customers waiting for long! We hope you enjoy! Happy Friday! - LWWF :) https://t.co/smyYriipxI\n",
            "\n",
            "we don't like to keep our lovely customers waiting for long we hope you enjoy happy friday lwwf :)\n",
            "\n",
            "nous peux \" c veux amener garder notre joli clients attendre pour longue nous espère vous savourer heureux samdi lwwf okk\n",
            "-----------------\n",
            "@Impatientraider On second thought, there’s just not enough time for a DD :) But new shorts entering system. Sheep must be buying.\n",
            "\n",
            "on second thought there ’ s just not enough time for a dd :) but new shorts entering system sheep must be buying\n",
            "\n",
            "sur deuxième pensé évident c c juste pas suffisamment temps pour un enfnat okk mais nouveau shorts entrant système brebis doit être acheter\n",
            "-----------------\n",
            "Jgh , but we have to go to Bayan :D bye\n",
            "\n",
            "jgh but we have to go to bayan :D bye\n",
            "\n",
            "jgh mais nous ont amener aller amener bayan : j aurevoir\n",
            "-----------------\n",
            "As an act of mischievousness, am calling the ETL layer of our in-house warehousing app Katamari.\n",
            "\n",
            "Well… as the name implies :p.\n",
            "\n",
            "as an act of mischievousness am calling the etl layer of our in-house warehousing app katamari well … as the name implies :p\n",
            "\n",
            "comme un acte de espièglerie suis appeler dans ged couche de notre dans ( maison entrepôts appli bomberman bien … comme dans nom sous-entend : p\n",
            "-----------------\n"
          ]
        }
      ],
      "source": [
        "for i in twitter_samples.strings('positive_tweets.json')[:10]:\n",
        "    print(i, process_tweet(i), translate(process_tweet(i)), sep='\\n\\n', end='\\n-----------------\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXMxWUtipDD8"
      },
      "source": [
        "Great!"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}